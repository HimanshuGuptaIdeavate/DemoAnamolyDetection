# WATCHTOWER - LSTM Configuration (v2 - Improved)
# M1: Bidirectional LSTM model for Ensemble (Parallel Voting with XGBoost)
# Input: 2Hz sequences (871, 10, 16) â€” 8 raw features + 8 intra-window deltas

# Data Paths
data:
  X_lstm_path: 'data/parquet/X_lstm.npy'
  y_path: 'data/parquet/y.npy'
  groups_path: 'data/parquet/groups_lstm.npy'

# Train-Test Split (MUST match XGBoost for ensemble alignment)
split:
  method: 'GroupKFold'
  n_splits: 4                    # Same as XGBoost
  random_state: 42               # Same as XGBoost

# LSTM Architecture
lstm:
  # Input shape: (10 timesteps, 16 features)
  units_1: 64                    # First LSTM layer units
  units_2: 32                    # Second LSTM layer units
  bidirectional: true            # Bidirectional reads forward + backward
  dropout: 0.4                   # Dropout after each LSTM layer (was 0.3)
  dense_units: 32                # Dense layer before output (was 16)
  dense_dropout: 0.3             # Dropout after dense layer
  activation: 'relu'             # Dense layer activation

# Training Parameters
training:
  epochs: 150                    # Max epochs (was 100, more room with higher dropout)
  batch_size: 16                 # Smaller batches = more gradient updates (was 32)
  learning_rate: 0.001           # Adam learning rate
  early_stopping_patience: 20    # More patience with higher dropout (was 15)
  reduce_lr_patience: 8          # Reduce LR if no improvement for 8 epochs (was 7)
  reduce_lr_factor: 0.5          # Reduce LR by half
  min_lr: 0.00001                # Minimum learning rate
  class_weight: 'auto'           # Computed as n_negative / n_positive
  validation_split: 0.15         # Validation split within training fold

# Evaluation
evaluation:
  probability_threshold: 0.10    # Same as XGBoost for fair comparison
  generate_plots: true

# Threshold Tuning (same F2-Score method as XGBoost)
threshold_tuning:
  enabled: true
  method: 'f2_score'
  save_plot: true

# MLflow Tracking
mlflow:
  tracking_uri: './mlruns'
  experiment_name: 'm1_lstm'
  run_name_prefix: 'lstm_groupkfold'
  log_model: true

# Artifacts Output
artifacts:
  model_dir: 'models'
  model_filename: 'lstm_model_{timestamp}.keras'
  report_dir: 'reports'
  plots_dir: 'reports/plots'

# ==============================================================================
# NOTES ON LSTM CONFIGURATION (v2)
# ==============================================================================
#
# Changes from v1:
#   - Features: 4 raw -> 8 raw + 8 deltas = 16 total
#     Raw: sinr_db, rsrp_dbm, rsrq_db, app_dl_mbps, prb_dl, prb_ul, mcs_dl, mcs_ul
#     Deltas: intra-window first differences for each raw feature
#
#   - Architecture: Bidirectional LSTM with BatchNormalization
#     Input (10, 16) -> BiLSTM(64) -> BN -> Dropout(0.4)
#     -> BiLSTM(32) -> BN -> Dropout(0.4)
#     -> Dense(32, relu) -> Dropout(0.3) -> Dense(1, sigmoid)
#
#   - Hyperparameters tuned for small dataset (871 samples):
#     dropout: 0.3 -> 0.4 (stronger regularization)
#     dense_units: 16 -> 32 (more capacity before output)
#     batch_size: 32 -> 16 (more gradient updates per epoch)
#     epochs: 100 -> 150 (more room with higher dropout)
#     early_stopping: 15 -> 20 (more patience with higher dropout)
#
# Key Design Decisions:
#   - Bidirectional: reads forward + backward, captures symmetric patterns
#   - BatchNormalization: stabilizes training, reduces internal covariate shift
#   - 8 raw features: covers signal quality + resource allocation + throughput
#   - Intra-window deltas: explicit rate-of-change within 5-sec windows
#   - Same GroupKFold splits as XGBoost: ensures ensemble alignment
#
# ==============================================================================
